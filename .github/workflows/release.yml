name: Compile, Build & Release WULU AI Base

on:
  push:
    tags: ['v*']

jobs:
  # --- 任务1: Windows (保持不变) ---
  build-windows-x64:
    name: Build for Windows (x64)
    runs-on: windows-latest
    steps:
      - name: Checkout repository with LFS
        uses: actions/checkout@v4
        with:
          lfs: true
      - name: Clone llama.cpp repository
        run: git clone https://github.com/ggerganov/llama.cpp.git
      - name: Compile llama.cpp with CMake
        shell: cmd
        run: |
          cd llama.cpp
          mkdir build
          cd build
          cmake .. -DBUILD_SHARED_LIBS=OFF -DLLAMA_STATIC=ON -DLLAMA_NATIVE=OFF -DLLAMA_CURL=OFF
          cmake --build . --config Release
      - name: Create staging directory
        run: mkdir staging
      - name: Copy files to staging
        run: |
          mkdir staging/bin
          cp llama.cpp/build/bin/Release/llama-server.exe staging/bin/
          cp -r model staging/
      - name: Install Inno Setup
        shell: powershell
        run: |
          Invoke-WebRequest -Uri "https://jrsoftware.org/download.php/is.exe" -OutFile "is-setup.exe"
          Start-Process -FilePath "is-setup.exe" -ArgumentList "/SP- /VERYSILENT /SUPPRESSMSGBOXES /NORESTART" -Wait
      - name: Add Inno Setup to PATH
        run: echo "C:\Program Files (x86)\Inno Setup 6" >> $env:GITHUB_PATH
      - name: Run Inno Setup Compiler
        run: iscc "packaging/windows/setup.iss"
      - name: Upload Windows Installer Artifact
        uses: actions/upload-artifact@v4
        with:
          name: installer-windows-x64
          path: packaging/windows/Output/*.exe

  # --- 任务2: Linux x64 ---
  build-linux-x64:
    name: Build for Linux (x64)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          lfs: true
      - name: Install build dependencies
        run: sudo apt-get update && sudo apt-get install -y cmake build-essential dos2unix
      - name: Clone llama.cpp repository
        run: git clone https://github.com/ggerganov/llama.cpp.git
      - name: Compile llama.cpp for x64
        run: |
          cd llama.cpp
          mkdir build && cd build
          cmake .. \
            -DLLAMA_STATIC=ON \
            -DLLAMA_NATIVE=OFF \
            -DLLAMA_CURL=OFF \
            -DCMAKE_C_FLAGS="-march=x86-64-v2" \  # 明确指定x86架构
            -DCMAKE_CXX_FLAGS="-march=x86-64-v2"
          cmake --build . --config Release
      - name: Create Debian package structure
        run: |
          mkdir -p staging/DEBIAN
          mkdir -p staging/opt/WuluAIBase/bin
          mkdir -p staging/opt/WuluAIBase/model
          mkdir -p staging/usr/share/applications
      - name: Prepare files for packaging
        run: |
          dos2unix packaging/linux/control_template
          # 修复：创建完整的control文件（包含架构字段）
          cat <<EOF > staging/DEBIAN/control
          $(cat packaging/linux/control_template)
          Architecture: amd64
          EOF
          
          cp llama.cpp/build/bin/llama-server staging/opt/WuluAIBase/bin/
          cp -r model/* staging/opt/WuluAIBase/model/
          cp packaging/linux/wulu-ai-base.desktop staging/usr/share/applications/
      - name: Generate .deb package
        run: dpkg-deb --build staging setup-wulu-ai-base-linux-x64-v${{ github.ref_name }}.deb
      - name: Upload Linux x64 Installer Artifact
        uses: actions/upload-artifact@v4
        with:
          name: installer-linux-x64
          path: ./*.deb

  # --- 任务3: Linux ARM64 (关键修复) ---
  build-linux-arm64:
    name: Build for Linux (ARM64)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          lfs: true
      - name: Install cross-compilation dependencies
        run: sudo apt-get update && sudo apt-get install -y cmake build-essential gcc-aarch64-linux-gnu g++-aarch64-linux-gnu dos2unix
      - name: Clone llama.cpp repository
        run: git clone https://github.com/ggerganov/llama.cpp.git
      - name: Cross-compile llama.cpp for ARM64
        run: |
          cd llama.cpp
          mkdir build && cd build
          cmake .. \
            -DCMAKE_SYSTEM_NAME=Linux \  # 明确指定目标系统
            -DCMAKE_SYSTEM_PROCESSOR=aarch64 \  # 关键修复：声明ARM架构
            -DCMAKE_C_COMPILER=aarch64-linux-gnu-gcc \
            -DCMAKE_CXX_COMPILER=aarch64-linux-gnu-g++ \
            -DLLAMA_STATIC=ON \
            -DLLAMA_CURL=OFF \
            -DLLAMA_NATIVE=OFF \
            -DCMAKE_C_FLAGS="-march=armv8-a" \  # 使用有效ARM架构
            -DCMAKE_CXX_FLAGS="-march=armv8-a"
          cmake --build . --config Release
      - name: Create Debian package structure
        run: |
          mkdir -p staging/DEBIAN
          mkdir -p staging/opt/WuluAIBase/bin
          mkdir -p staging/opt/WuluAIBase/model
          mkdir -p staging/usr/share/applications
      - name: Prepare files for ARM packaging
        run: |
          dos2unix packaging/linux/control_template
          # 修复：创建完整的control文件（包含架构字段）
          cat <<EOF > staging/DEBIAN/control
          $(cat packaging/linux/control_template)
          Architecture: arm64
          EOF

          cp llama.cpp/build/bin/llama-server staging/opt/WuluAIBase/bin/
          cp -r model/* staging/opt/WuluAIBase/model/
          cp packaging/linux/wulu-ai-base.desktop staging/usr/share/applications/
      - name: Generate .deb package for ARM64
        run: dpkg-deb --build staging setup-wulu-ai-base-linux-arm64-v${{ github.ref_name }}.deb
      - name: Upload Linux ARM64 Installer Artifact
        uses: actions/upload-artifact@v4
        with:
          name: installer-linux-arm64
          path: ./*.deb