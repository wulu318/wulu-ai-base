name: Compile, Build & Release WULU AI Base

on:
  push:
    tags: ['v*']

jobs:
  # --- Windows构建修复 ---
  build-windows-x64:
    runs-on: windows-latest
    steps:
      - uses: actions/checkout@v4
        with:
          lfs: true
          submodules: recursive  # 关键修复：递归克隆子模块
      
      # 删除克隆步骤（直接用checkout的子模块）
      - name: Compile llama.cpp
        shell: bash
        run: |
          cd llama.cpp
          mkdir build
          cd build
          cmake .. -DBUILD_SHARED_LIBS=OFF -DLLAMA_STATIC=ON -DLLAMA_NATIVE=OFF -DLLAMA_CURL=OFF -DLLAMA_SERVER=ON
          cmake --build . --config Release --target server

      - name: Prepare staging
        run: |
          mkdir -p staging/bin
          mkdir -p staging/model
          cp llama.cpp/build/bin/Release/server.exe staging/bin/llama-server.exe

      - name: Install Inno Setup
        uses: pinage404/innosetup-action@v1
        with:
          version: '6.2.2'

      - name: Compile installer
        run: iscc "packaging/windows/setup.iss"
        working-directory: ${{ github.workspace }}

      - uses: actions/upload-artifact@v4
        with:
          name: installer-windows-x64
          path: packaging/windows/Output/*.exe

  # --- Linux x64修复 ---
  build-linux-x64:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          lfs: true
          submodules: recursive
      
      - name: Install dependencies
        run: sudo apt-get update && sudo apt-get install -y build-essential cmake dos2unix

      - name: Compile llama.cpp
        run: |
          cd llama.cpp
          mkdir build
          cd build
          cmake .. -DLLAMA_STATIC=ON -DLLAMA_NATIVE=OFF -DLLAMA_CURL=OFF -DLLAMA_SERVER=ON
          cmake --build . --config Release --target server

      - name: Prepare staging
        run: |
          mkdir -p staging/DEBIAN
          mkdir -p staging/opt/WuluAIBase/{bin,model}
          mkdir -p staging/usr/share/applications
          
          cp packaging/linux/control_template staging/DEBIAN/control
          echo "Architecture: amd64" >> staging/DEBIAN/control
          cp llama.cpp/build/bin/server staging/opt/WuluAIBase/bin/llama-server
          chmod +x staging/opt/WuluAIBase/bin/llama-server

      - name: Build DEB package
        run: dpkg-deb --build staging wulu-ai-base_${{ github.ref_name }}_amd64.deb

      - uses: actions/upload-artifact@v4
        with:
          name: installer-linux-x64
          path: '*.deb'

  # --- ARM64交叉编译修复 ---
  build-linux-arm64:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          lfs: true
          submodules: recursive
      
      - name: Set up cross-compiler
        run: |
          sudo apt-get update
          sudo apt-get install -y gcc-aarch64-linux-gnu g++-aarch64-linux-gnu

      - name: Cross-compile
        run: |
          cd llama.cpp
          mkdir build-arm64
          cd build-arm64
          cmake .. \
            -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchain/arm64.cmake \
            -DLLAMA_STATIC=ON \
            -DLLAMA_SERVER=ON
          cmake --build . --config Release --target server

      - name: Prepare ARM staging
        run: |
          mkdir -p staging/DEBIAN
          mkdir -p staging/opt/WuluAIBase/{bin,model}
          mkdir -p staging/usr/share/applications
          
          cp packaging/linux/control_template staging/DEBIAN/control
          echo "Architecture: arm64" >> staging/DEBIAN/control
          cp llama.cpp/build-arm64/bin/server staging/opt/WuluAIBase/bin/llama-server
          chmod +x staging/opt/WuluAIBase/bin/llama-server

      - name: Build ARM DEB
        run: dpkg-deb --build staging wulu-ai-base_${{ github.ref_name }}_arm64.deb

      - uses: actions/upload-artifact@v4
        with:
          name: installer-linux-arm64
          path: '*.deb'