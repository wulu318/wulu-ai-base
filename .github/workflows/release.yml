# 工作流名称
name: Compile, Build & Release WULU AI Base

# 触发条件
on:
  push:
    tags: ['v*']

# 任务列表
jobs:
  # --- 任务1: Windows ---
  build-windows-x64:
    name: Build for Windows (x64)
    runs-on: windows-latest
    steps:
      - name: Checkout repository with LFS
        uses: actions/checkout@v4
        with:
          lfs: true
      # 修正: 使用正确的仓库地址
      - name: Clone llama.cpp repository
        run: git clone https://github.com/ggml-org/llama.cpp.git
      - name: Compile llama.cpp with CMake
        shell: cmd
        run: |
          cd llama.cpp
          mkdir build
          cd build
          cmake .. -DBUILD_SHARED_LIBS=OFF -DLLAMA_STATIC=ON -DLLAMA_NATIVE=OFF -DLLAMA_CURL=OFF
          cmake --build . --config Release
      - name: Create staging directory
        run: mkdir staging
      - name: Copy files to staging
        run: |
          mkdir staging/bin
          cp llama.cpp/build/bin/Release/llama-server.exe staging/bin/
          cp -r model staging/
      - name: Install Inno Setup
        shell: powershell
        run: |
          Invoke-WebRequest -Uri "https://jrsoftware.org/download.php/is.exe" -OutFile "is-setup.exe"
          Start-Process -FilePath "is-setup.exe" -ArgumentList "/SP- /VERYSILENT /SUPPRESSMSGBOXES /NORESTART" -Wait
      - name: Add Inno Setup to PATH
        run: echo "C:\Program Files (x86)\Inno Setup 6" >> $env:GITHUB_PATH
      - name: Run Inno Setup Compiler
        run: iscc "packaging/windows/setup.iss"
      - name: Upload Windows Installer Artifact
        uses: actions/upload-artifact@v4
        with:
          name: installer-windows-x64
          path: packaging/windows/Output/*.exe

  # --- 任务2: Linux x64 ---
  build-linux-x64:
    name: Build for Linux (x64)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          lfs: true
      - name: Install build dependencies
        run: sudo apt-get update && sudo apt-get install -y cmake build-essential dos2unix
      # 修正: 使用正确的仓库地址
      - name: Clone llama.cpp repository
        run: git clone https://github.com/ggml-org/llama.cpp.git
      - name: Compile llama.cpp for x64
        run: cd llama.cpp && mkdir build && cd build && cmake .. -DLLAMA_STATIC=ON -DLLAMA_NATIVE=OFF -DLLAMA_CURL=OFF && cmake --build . --config Release
      - name: Create Debian package structure
        run: |
          mkdir -p staging/DEBIAN
          mkdir -p staging/opt/WuluAIBase/bin
          mkdir -p staging/opt/WuluAIBase/model
          mkdir -p staging/usr/share/applications
      - name: Prepare files for packaging
        run: |
          dos2unix packaging/linux/control_template
          cp packaging/linux/control_template staging/DEBIAN/control
          echo "Architecture: amd64" >> staging/DEBIAN/control
          cp llama.cpp/build/bin/llama-server staging/opt/WuluAIBase/bin/
          cp -r model/* staging/opt/WuluAIBase/model/
          cp packaging/linux/wulu-ai-base.desktop staging/usr/share/applications/
      - name: Generate .deb package
        run: dpkg-deb --build staging setup-wulu-ai-base-linux-x64-v${{ github.ref_name }}.deb
      - name: Upload Linux x64 Installer Artifact
        uses: actions/upload-artifact@v4
        with:
          name: installer-linux-x64
          path: ./*.deb

  # --- 任务3: Linux ARM64 ---
  build-linux-arm64:
    name: Build for Linux (ARM64)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          lfs: true
      - name: Install cross-compilation dependencies
        run: sudo apt-get update && sudo apt-get install -y cmake build-essential gcc-aarch64-linux-gnu g++-aarch64-linux-gnu dos2unix
      # 修正: 使用正确的仓库地址
      - name: Clone llama.cpp repository
        run: git clone https://github.com/ggml-org/llama.cpp.git
      - name: Cross-compile llama.cpp for ARM64
        # 终极修正: 直接在cmake命令中强制指定ARM CPU架构
        run: |
          cd llama.cpp
          mkdir build && cd build
          cmake .. -DCMAKE_C_COMPILER=aarch64-linux-gnu-gcc -DCMAKE_CXX_COMPILER=aarch64-linux-gnu-g++ -DCMAKE_C_FLAGS="-march=armv8-a" -DCMAKE_CXX_FLAGS="-march=armv8-a" -DLLAMA_STATIC=ON -DLLAMA_CURL=OFF
          cmake --build . --config Release
      - name: Create Debian package structure
        run: |
          mkdir -p staging/DEBIAN
          mkdir -p staging/opt/WuluAIBase/bin
          mkdir -p staging/opt/WuluAIBase/model
          mkdir -p staging/usr/share/applications
      - name: Prepare files for ARM packaging
        run: |
          dos2unix packaging/linux/control_template
          cp packaging/linux/control_template staging/DEBIAN/control
          echo "Architecture: arm64" >> staging/DEBIAN/control
          cp llama.cpp/build/bin/llama-server staging/opt/WuluAIBase/bin/
          cp -r model/* staging/opt/WuluAIBase/model/
          cp packaging/linux/wulu-ai-base.desktop staging/usr/share/applications/
      - name: Generate .deb package for ARM64
        run: dpkg-deb --build staging setup-wulu-ai-base-linux-arm64-v${{ github.ref_name }}.deb
      - name: Upload Linux ARM64 Installer Artifact
        uses: actions/upload-artifact@v4
        with:
          name: installer-linux-arm64
          path: ./*.deb